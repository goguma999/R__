# 7️⃣워드 클라우드 그래프 
### ✅ 워드 클라우드 그래프를 사용하는 경우?  
1. 텍스트 데이터 탐색 및 요약 그리고 주요 단어 강조 등을 시각화 하고 싶을 때  
2. 트렌드를 분석하거나 소셜 미디어와 뉴스에서 언급하는 주요 내용들을 분석할 때    
3. 마케팅에서 고객 만족도 등을 조사할 때    
4. 문서나 문학 작품을 분석하고자 할 때

&nbsp;

### ✅ Rjava 설치
```r
1. java 64비트 다운
https://www.java.com/en/download/manual.jsp

2. Windows Offline (64-bit) 으로 설치

3. 아래와 같이 환경설정을 한다.
(R스튜디오) Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0')(c드라이브자바위치)
※ 설명 : R 에서 지금 자바홈 위치가 어디있다는 것을 알려주는 것

4. 설치를 한다
install.packages("rJava")

library(rJava)
```

&nbsp;


### 📍Code1. wordcloud2패키지_레이디 버그 게시판의 게시글(한글 데이터)
```r
# 배운 사람들이 쓰는 세련된 워드클라우드 
install.packages("wordcloud2")

# 라이브러리 로드
library(wordcloud2)
library(RColorBrewer)
library(plyr)
library(data.table)

# 텍스트 데이터 로드
txt <- readLines('ladybug5.txt', encoding = "UTF-8")

# 특수문자 제거 및 공백 처리
# 불필요한 텍스트를 제거 |n요런거. 
cleaned_txt <- iconv(txt, "UTF-8", "UTF-8", sub="")

# 한글, 숫자, 공백을 제외한 모든 특수문자를 공백으로 대체
cleaned_txt <- gsub("[^[:alnum:][:space:]ㄱ-ㅎㅏ-ㅣ가-힣]", " ", cleaned_txt)

# 연속된 공백을 하나의 공백으로 변환
cleaned_txt <- gsub("\\s+", " ", cleaned_txt)  

# 명사 추출 함수 
extract_nouns_simple <- function(doc) {
  doc <- as.character(doc) # 문자로 변환
  words <- unlist(strsplit(doc, "\\s+"))  # 공백을 기준으로 단어 분리
  nouns <- Filter(function(x) {grepl("^[가-힣]+$", x) && nchar(x) >= 2}, words)
	#	grep에서의 꺽쇠^: '시작'┘     한글로만 구성된 단어 추출 및 길이 2 이상 필터링
  return(nouns)                       
}

# 명사 추출
nouns <- extract_nouns_simple(cleaned_txt)

# 추출된 명사 확인
print(head(nouns, 10))  # 상위 10개 단어 확인

# 단어 빈도수 계산
word_freq <- table(nouns)
word_freq <- as.data.frame(word_freq, stringsAsFactors = FALSE)
word_freq <- arrange(word_freq, desc(Freq))   # 명사의 빈도수를 내림차순으로 정렬

# 상위 10개 단어 확인
print(head(word_freq, 10))

# 유효하지 않은 값 확인 및 제거
# 빈 문자열을 포함하는 행을 제거합니다. 
word_freq <- word_freq[word_freq$nouns != "", ]

# 단어 빈도수 데이터프레임 확인
print(head(word_freq, 10))

# 특정 단어 제외하기
word_freq <- subset( word_freq, nouns != "너무")
word_freq <- subset( word_freq, nouns != "빨리")

# 워드클라우드 생성 (하트 모양)
wordcloud2(data = word_freq, shape = "heart", color = brewer.pal(8, "Dark2"))
```

<img src="https://github.com/goguma999/R__/blob/main/_7-1.png">


**<기본 제공 모양>**
```r
circle: 원형 (기본값)
cardioid: 심장 모양
diamond: 다이아몬드 모양
triangle-forward: 정삼각형 (앞쪽)
triangle: 정삼각형 (뒤쪽)
pentagon: 오각형
star: 별 모양
```

&nbsp;


### 📍Code2. wordcloud2패키지_겨울왕국 대본(영문 데이터)
```r
install.packages("wordcloud2")
install.packages("tm")           # 텍스트 데이터 전처리 전문 패키지
install.packages("RColorBrewer") 
install.packages("plyr")         # 데이터 조작을 위한 패키지
install.packages("data.table")   

# 라이브러리 로드
library(wordcloud2)
library(tm)
library(RColorBrewer)
library(plyr)
library(data.table)

# 텍스트 데이터 로드
file_path <- "winter.txt"
txt <- readLines(file_path, encoding = "UTF-8")

# 특수문자 제거 및 공백 처리
# 불필요한 텍스트 제거
cleaned_txt <- iconv(txt, "UTF-8", "UTF-8", sub="")
# 특수문자 제거 
cleaned_txt <- gsub("[^[:alnum:][:space:]]", " ", cleaned_txt)
# 연속된 공백문자를 하나의 공백으로 
cleaned_txt <- gsub("\\s+", " ", cleaned_txt)  

# 텍스트를 Corpus로 변환 ( 텍스트 마이닝 패키지의 corpus 객체로 변환 )
corpus <- Corpus(VectorSource(cleaned_txt))

# 데이터 전처리
corpus <- tm_map(corpus, content_transformer(tolower))  # 소문자로 변환
corpus <- tm_map(corpus, removePunctuation)  # 구두점 제거(.,?!...)
corpus <- tm_map(corpus, removeNumbers)  # 숫자 제거
corpus <- tm_map(corpus, removeWords, stopwords("en")) # 영어 불용어 제거(am, i, a, the..)
corpus <- tm_map(corpus, stripWhitespace)  # 여백 제거

# 단어 행렬 생성
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
word_freqs <- sort(rowSums(m), decreasing = TRUE) #단어의 빈도수가 높은 순으로 정렬
word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)

# anna, kristoff contd 제외
corpus <- tm_map(corpus, removeWords, c(stopwords("en"),"anna","kristoff","contd") )

# 워드 클라우드 생성 (하트 모양)
wordcloud2(data = word_freq_df, shape = "heart", color = brewer.pal(8, "Dark2"))
```

<img src="https://github.com/goguma999/R__/blob/main/_7-2.png">


&nbsp;


### ✅ 감정분석 데이터 분석

1. 워드 클라우드 시각화 
2. 표형태 긍정, 부정 단어의 순위를 나열 하는 시각화 
- 관련된 대표적인 사이트 : 썸트렌드 사이트
- 나의 데이터 분석을 위한 더 정교한 시각화를 하고 싶다면 R과 파이썬 코드 구현


### 📍Code3. wordcloud2패키지_조 바이든 연설문의 긍정 & 부정 단어
```r
# 패키지 설치 및 라이브러리 로드
install.packages("wordcloud2")
install.packages("tm")  # 문자를 정제하고 전처리하기 위해 필요
install.packages("RColorBrewer") 
install.packages("htmlwidgets") # html 위젯을 생성하기 위해 필요 
install.packages("webshot")  # html 파일을 png그림파일로 저장하기 위해 필요
install.packages("magick")   # 이미지 파일을 읽고 변환하기 위한 패키지 
install.packages("ggplot2")  
install.packages("grid")     # 두 개의 그래프를 하나의 화면에 출력하기 위해 필요

library(wordcloud2)
library(tm)
library(RColorBrewer)
library(htmlwidgets)
library(webshot)
library(magick)
library(ggplot2)
library(grid)

# PhantomJS 설치
webshot::install_phantomjs() # webshot 패키지를 위해 필요한 패키지 

# 텍스트 데이터 로드
biden_file_path <- "joe.txt"
biden_txt <- readLines(biden_file_path, encoding = "UTF-8")

# 특수문자 제거 및 공백 처리
cleaned_txt <- iconv(biden_txt, "UTF-8", "UTF-8", sub="")
cleaned_txt <- gsub("[^[:alnum:][:space:]]", " ", cleaned_txt) #문자나 숫자 아닌 것 제거 
cleaned_txt <- gsub("\\s+", " ", cleaned_txt) # 두 번이상의 공백문자 제거 

# 텍스트를 Corpus로 변환
corpus <- VCorpus(VectorSource(cleaned_txt))

# 데이터 전처리
corpus <- tm_map(corpus, content_transformer(tolower)) # 소문자 변환
corpus <- tm_map(corpus, removePunctuation)            # 구두점 제거    
corpus <- tm_map(corpus, removeNumbers)                # 숫자 제거 
corpus <- tm_map(corpus, removeWords, stopwords("en")) # 불용어 제거 
corpus <- tm_map(corpus, stripWhitespace)              # 공백만 있는 행 제거 

# 단어 행렬 생성
dtm <- TermDocumentMatrix(corpus)  # 단어 행렬 만들기 
m <- as.matrix(dtm)                # 행렬형태로 변환
word_freqs <- sort(rowSums(m), decreasing = TRUE)  # 빈도수 높은 단어로 정렬
word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)
word_freq_Df

# 상위 100개의 단어만 선택
word_freq_df <- head(word_freq_df, 100)

# 워드 클라우드 생성
word_cloud <- wordcloud2(data = word_freq_df, shape = "circle", color = brewer.pal(8, "Dark2"))

# htmlwidgets로 워드 클라우드를 저장
saveWidget(word_cloud, "biden_cloud.html", selfcontained = TRUE)

# 워드 클라우드를 이미지로 저장
webshot("biden_cloud.html", file = "biden_cloud.png", delay = 10)

# magick으로 이미지 불러오기
biden_img <- image_read("biden_cloud.png")

# ggplot으로 이미지 변환
biden_plot <- ggplot() + 
  annotation_custom(rasterGrob(biden_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
  theme_void()

# 플롯 출력
print(biden_plot)

c드라이브-data 밑에서 png 확인 가능 
```

![word_freq_df의 형태](https://prod-files-secure.s3.us-west-2.amazonaws.com/08691aea-b5b9-4275-80cd-5d0d824962f4/5cf86864-e2b7-4054-9fd3-fa03b7bf492e/Untitled.png)

word_freq_df의 형태

▪️문제130. 트럼프 연설문을 워드 클라우드로 시각화 하시오 ! 

- 코드
    
    ```r
    # 패키지 설치 및 라이브러리 로드
    install.packages("wordcloud2")
    install.packages("tm")  # 문자를 정제하고 전처리하기 위해 필요
    install.packages("RColorBrewer") 
    install.packages("htmlwidgets") # html 위젯을 생성하기 위해 필요 
    install.packages("webshot")  # html 파일을 png그림파일로 저장하기 위해 필요
    install.packages("magick")   # 이미지 파일을 읽고 변환하기 위한 패키지 
    install.packages("ggplot2")  
    install.packages("grid")     # 두 개의 그래프를 하나의 화면에 출력하기 위해 필요
    
    library(wordcloud2)
    library(tm)
    library(RColorBrewer)
    library(htmlwidgets)
    library(webshot)
    library(magick)
    library(ggplot2)
    library(grid)
    
    # PhantomJS 설치
    webshot::install_phantomjs() # webshot 패키지를 위해 필요한 패키지 
    
    # 텍스트 데이터 로드
    biden_file_path <- "joe.txt"
    biden_txt <- readLines(biden_file_path, encoding = "UTF-8")
    
    # 특수문자 제거 및 공백 처리
    cleaned_txt <- iconv(biden_txt, "UTF-8", "UTF-8", sub="")
    cleaned_txt <- gsub("[^[:alnum:][:space:]]", " ", cleaned_txt) #문자나 숫자 아닌 것 제거 
    cleaned_txt <- gsub("\\s+", " ", cleaned_txt) # 두 번이상의 공백문자 제거 
    
    # 텍스트를 Corpus로 변환
    corpus <- VCorpus(VectorSource(cleaned_txt))
    
    # 데이터 전처리
    corpus <- tm_map(corpus, content_transformer(tolower)) # 소문자 변환
    corpus <- tm_map(corpus, removePunctuation)            # 구두점 제거    
    corpus <- tm_map(corpus, removeNumbers)                # 숫자 제거 
    corpus <- tm_map(corpus, removeWords, stopwords("en")) # 불용어 제거 
    corpus <- tm_map(corpus, stripWhitespace)              # 공백만 있는 행 제거 
    
    # 단어 행렬 생성
    dtm <- TermDocumentMatrix(corpus)  # 단어 행렬 만들기 
    m <- as.matrix(dtm)                # 행렬형태로 변환
    word_freqs <- sort(rowSums(m), decreasing = TRUE)  # 빈도수 높은 단어로 정렬
    word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)
    word_freq_Df
    
    # 상위 100개의 단어만 선택
    word_freq_df <- head(word_freq_df, 100)
    
    # 워드 클라우드 생성
    word_cloud <- wordcloud2(data = word_freq_df, shape = "circle", color = brewer.pal(8, "Dark2"))
    
    # htmlwidgets로 워드 클라우드를 저장  # 저장해놓고 이미지 뿌려주는게 더 잘 나옴
    saveWidget(word_cloud, "biden_cloud.html", selfcontained = TRUE)
    
    # 워드 클라우드를 이미지로 저장
    webshot("biden_cloud.html", file = "biden_cloud.png", delay = 10)
    
    # magick으로 이미지 불러오기
    biden_img <- image_read("biden_cloud.png")
    
    # ggplot으로 이미지 변환
    biden_plot <- ggplot() + 
      annotation_custom(rasterGrob(biden_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
      theme_void()
    
    # 플롯 출력
    print(biden_plot)
    
    c드라이브-data 밑에서 png 확인 가능 
    ```
    

[trump_cloud.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/08691aea-b5b9-4275-80cd-5d0d824962f4/88ce66c8-2e03-4e6b-8aa9-bc3d6e85ff1d/trump_cloud.png)

▪️문제131. 조바이든 연설문에는 긍정 단어가 얼마나 나오는지 워드 클라우드로 시각화하시오 

- [데이터](https://cafe.daum.net/oracleoracle/Soei/45): 영문 긍정단어, 영문 부정단어
- 코드
    
    ```r
    # 패키지 설치 및 라이브러리 로드
    install.packages("wordcloud2")
    install.packages("tm")
    install.packages("RColorBrewer")
    install.packages("htmlwidgets")
    install.packages("webshot")
    install.packages("magick")
    install.packages("ggplot2")
    install.packages("grid")
    
    library(wordcloud2)
    library(tm)
    library(RColorBrewer)
    library(htmlwidgets)
    library(webshot)
    library(magick)
    library(ggplot2)
    library(grid)
    
    # PhantomJS 설치
    webshot::install_phantomjs()
    
    # 텍스트 데이터 로드
    setwd('c:\\data')
    biden_file_path <- "joe.txt"
    **positive_words_file <- "positive-words.txt"** 
    
    biden_txt <- readLines(biden_file_path, encoding = "UTF-8")
    **# 긍정단어 불러오기**
    **positive_words <- readLines(positive_words_file, encoding = "UTF-8")**
    
    # 특수문자 제거 및 공백 처리
    cleaned_txt <- iconv(biden_txt, "UTF-8", "UTF-8", sub="")
    cleaned_txt <- gsub("[^[:alnum:][:space:]]", " ", cleaned_txt)
    cleaned_txt <- gsub("\\s+", " ", cleaned_txt)
    
    # 텍스트를 Corpus로 변환
    corpus <- VCorpus(VectorSource(cleaned_txt))
    
    # 데이터 전처리
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeNumbers)
    corpus <- tm_map(corpus, removeWords, stopwords("en"))
    corpus <- tm_map(corpus, stripWhitespace)
    
    # 단어 행렬 생성
    dtm <- TermDocumentMatrix(corpus)
    m <- as.matrix(dtm)
    word_freqs <- sort(rowSums(m), decreasing = TRUE)
    word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)
    
    **# 긍정 단어 필터링
    positive_word_freq_df <- word_freq_df[word_freq_df$word %in% positive_words, ]**
    
    # 상위 100개의 단어만 선택
    positive_word_freq_df <- head(positive_word_freq_df, 100)
    
    # 워드 클라우드 생성
    positive_cloud <- wordcloud2(data = positive_word_freq_df, shape = "circle", color = brewer.pal(8, "Dark2"))
    
    # htmlwidgets로 워드 클라우드를 저장
    saveWidget(positive_cloud, "positive_cloud.html", selfcontained = TRUE)
    
    # 워드 클라우드를 이미지로 저장
    webshot("positive_cloud.html", file = "positive_cloud.png", delay = 10)
    
    # magick으로 이미지 불러오기
    positive_img <- image_read("positive_cloud.png")
    
    # ggplot으로 이미지 변환
    positive_plot <- ggplot() + 
      annotation_custom(rasterGrob(positive_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
      theme_void()
    
    # 플롯 출력
    print(positive_plot)
    ```
    

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/08691aea-b5b9-4275-80cd-5d0d824962f4/737b265a-5455-4810-b763-8ff9a2620a1d/Untitled.png)

▪️문제132. 트럼프 연설문의 긍정단어로 워드 클라우드를 그리시오! 

- 코드
    
    ```r
    # 패키지 설치 및 라이브러리 로드
    install.packages("wordcloud2")
    install.packages("tm")
    install.packages("RColorBrewer")
    install.packages("htmlwidgets")
    install.packages("webshot")
    install.packages("magick")
    install.packages("ggplot2")
    install.packages("grid")
    
    library(wordcloud2)
    library(tm)
    library(RColorBrewer)
    library(htmlwidgets)
    library(webshot)
    library(magick)
    library(ggplot2)
    library(grid)
    
    # PhantomJS 설치
    webshot::install_phantomjs()
    
    # 텍스트 데이터 로드
    setwd('c:\\data')
    trump_file_path <- "trump.txt"
    positive_words_file <- "positive-words.txt" 
    
    trump_txt <- readLines(trump_file_path, encoding = "UTF-8")
    # 긍정단어 불러오기
    positive_words <- readLines(positive_words_file, encoding = "UTF-8")
    
    # 특수문자 제거 및 공백 처리
    cleaned_txt <- iconv(trump_txt, "UTF-8", "UTF-8", sub="")
    cleaned_txt <- gsub("[^[:alnum:][:space:]]", " ", cleaned_txt)
    cleaned_txt <- gsub("\\s+", " ", cleaned_txt)
    
    # 텍스트를 Corpus로 변환
    corpus <- VCorpus(VectorSource(cleaned_txt))
    
    # 데이터 전처리
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeNumbers)
    corpus <- tm_map(corpus, removeWords, stopwords("en"))
    corpus <- tm_map(corpus, stripWhitespace)
    
    # 단어 행렬 생성
    dtm <- TermDocumentMatrix(corpus)
    m <- as.matrix(dtm)
    word_freqs <- sort(rowSums(m), decreasing = TRUE)
    word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)
    
    # 긍정 단어 필터링
    positive_word_freq_df <- word_freq_df[word_freq_df$word %in% positive_words, ]
    
    # 상위 100개의 단어만 선택
    positive_word_freq_df <- head(positive_word_freq_df, 100)
    
    # 워드 클라우드 생성
    positive_cloud <- wordcloud2(data = positive_word_freq_df, shape = "circle", color = brewer.pal(8, "Dark2"))
    
    # htmlwidgets로 워드 클라우드를 저장
    saveWidget(positive_cloud, "positive_cloud.html", selfcontained = TRUE)
    
    # 워드 클라우드를 이미지로 저장
    webshot("positive_cloud.html", file = "positive_cloud.png", delay = 10)
    
    # magick으로 이미지 불러오기
    positive_img <- image_read("positive_cloud.png")
    
    # ggplot으로 이미지 변환
    positive_plot <- ggplot() + 
      annotation_custom(rasterGrob(positive_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
      theme_void()
    
    # 플롯 출력
    print(positive_plot)
    ```
    

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/08691aea-b5b9-4275-80cd-5d0d824962f4/e81d56b8-3a04-4c3d-8c59-e5f053d11e17/Untitled.png)

▪️문제133. 트럼프 연설문에서 긍정단어와 부정단어를 워드 클라우드로 그리는데 하나의 화면에 바로 같이 출력 되게 하시오 ! 

- 코드
    
    ```r
    # 패키지 설치 및 라이브러리 로드
    install.packages("wordcloud2")
    install.packages("tm")
    install.packages("RColorBrewer")
    install.packages("plyr")
    install.packages("data.table")
    **install.packages("patchwork") # ggplot 객체를 하나의 화면에 배치하기 위해 필요**
    install.packages("htmlwidgets")
    install.packages("webshot")
    install.packages("magick")
    install.packages("ggplot2")
    install.packages("grid")
    
    library(wordcloud2)
    library(tm)
    library(RColorBrewer)
    library(plyr)
    library(data.table)
    library(patchwork)
    library(htmlwidgets)
    library(webshot)
    library(magick)
    library(ggplot2)
    library(grid)
    
    # PhantomJS 설치
    webshot::install_phantomjs()
    
    # 텍스트 데이터 로드
    **biden_file_path <- "trump.txt"**
    positive_words_file <- "positive-words.txt"
    negative_words_file <- "negative-words.txt"
    
    biden_txt <- readLines(biden_file_path, encoding = "UTF-8")
    positive_words <- readLines(positive_words_file, encoding = "UTF-8")
    negative_words <- readLines(negative_words_file, encoding = "UTF-8")
    
    # 특수문자 제거 및 공백 처리
    cleaned_txt <- iconv(biden_txt, "UTF-8", "UTF-8", sub="")
    cleaned_txt <- gsub("[^[:alnum:][:space:]]", " ", cleaned_txt)
    cleaned_txt <- gsub("\\s+", " ", cleaned_txt)
    
    # 텍스트를 Corpus로 변환
    corpus <- VCorpus(VectorSource(cleaned_txt))
    
    # 데이터 전처리
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeNumbers)
    corpus <- tm_map(corpus, removeWords, stopwords("en"))
    corpus <- tm_map(corpus, stripWhitespace)
    
    # 단어 행렬 생성
    dtm <- TermDocumentMatrix(corpus)
    m <- as.matrix(dtm)
    word_freqs <- sort(rowSums(m), decreasing = TRUE)
    word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)
    
    # 긍정 단어 및 부정 단어 필터링
    positive_word_freq_df <- word_freq_df[word_freq_df$word %in% positive_words, ]
    negative_word_freq_df <- word_freq_df[word_freq_df$word %in% negative_words, ]
    
    # 상위 100개의 단어만 선택
    positive_word_freq_df <- head(positive_word_freq_df, 100)
    negative_word_freq_df <- head(negative_word_freq_df, 100)
    
    # 워드 클라우드 생성
    positive_cloud <- wordcloud2(data = positive_word_freq_df, shape = "circle", color = brewer.pal(8, "Dark2"))
    negative_cloud <- wordcloud2(data = negative_word_freq_df, shape = "circle", color = brewer.pal(8, "Reds"))
    
    # htmlwidgets로 각각의 워드 클라우드를 저장
    saveWidget(positive_cloud, "positive_cloud.html", selfcontained = TRUE)
    saveWidget(negative_cloud, "negative_cloud.html", selfcontained = TRUE)
    
    # 두 개의 htmlwidgets을 한 화면에 표시
    webshot("positive_cloud.html", file = "positive_cloud.png", delay = 10)
    webshot("negative_cloud.html", file = "negative_cloud.png", delay = 10)
    
    # magick으로 이미지 불러오기
    positive_img <- image_read("positive_cloud.png")
    negative_img <- image_read("negative_cloud.png")
    
    # ggplot으로 이미지 변환
    positive_plot <- ggplot() + 
      annotation_custom(rasterGrob(positive_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
      theme_void()
    
    negative_plot <- ggplot() + 
      annotation_custom(rasterGrob(negative_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
      theme_void()
    
    # patchwork로 두 이미지를 하나의 화면에 표시
    combined_plot <- positive_plot + negative_plot
    
    # 플롯 출력
    print(combined_plot)
    
    ```
    

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/08691aea-b5b9-4275-80cd-5d0d824962f4/802ae315-3b43-4a38-b95f-258126fe0f42/Untitled.png)

▪️문제134. 조바이든 연설문의 긍정과 부정 단어를 워드 클라우드로 그리는데 하나의 화면에 바로 같이 출력 되게 하시오 ! 

- 코드
    
    ```r
    # 패키지 설치 및 라이브러리 로드
    install.packages("wordcloud2")
    install.packages("tm")
    install.packages("RColorBrewer")
    install.packages("plyr")
    install.packages("data.table")
    **install.packages("patchwork") # ggplot 객체를 하나의 화면에 배치하기 위해 필요**
    install.packages("htmlwidgets")
    install.packages("webshot")
    install.packages("magick")
    install.packages("ggplot2")
    install.packages("grid")
    
    library(wordcloud2)
    library(tm)
    library(RColorBrewer)
    library(plyr)
    library(data.table)
    library(patchwork)
    library(htmlwidgets)
    library(webshot)
    library(magick)
    library(ggplot2)
    library(grid)
    
    # PhantomJS 설치
    webshot::install_phantomjs()
    
    # 텍스트 데이터 로드
    **biden_file_path <- "joe.txt"**
    positive_words_file <- "positive-words.txt"
    negative_words_file <- "negative-words.txt"
    
    **joe**_txt <- readLines(biden_file_path, encoding = "UTF-8")
    positive_words <- readLines(positive_words_file, encoding = "UTF-8")
    negative_words <- readLines(negative_words_file, encoding = "UTF-8")
    
    # 특수문자 제거 및 공백 처리
    cleaned_txt <- iconv(joe_txt, "UTF-8", "UTF-8", sub="")
    cleaned_txt <- gsub("[^[:alnum:][:space:]]", " ", cleaned_txt)
    cleaned_txt <- gsub("\\s+", " ", cleaned_txt)
    
    # 텍스트를 Corpus로 변환
    corpus <- VCorpus(VectorSource(cleaned_txt))
    
    # 데이터 전처리
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeNumbers)
    corpus <- tm_map(corpus, removeWords, stopwords("en"))
    corpus <- tm_map(corpus, stripWhitespace)
    
    # 단어 행렬 생성
    dtm <- TermDocumentMatrix(corpus)
    m <- as.matrix(dtm)
    word_freqs <- sort(rowSums(m), decreasing = TRUE)
    word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)
    
    # 긍정 단어 및 부정 단어 필터링
    positive_word_freq_df <- word_freq_df[word_freq_df$word %in% positive_words, ]
    negative_word_freq_df <- word_freq_df[word_freq_df$word %in% negative_words, ]
    
    # 상위 100개의 단어만 선택
    positive_word_freq_df <- head(positive_word_freq_df, 100)
    negative_word_freq_df <- head(negative_word_freq_df, 100)
    
    # 워드 클라우드 생성
    positive_cloud <- wordcloud2(data = positive_word_freq_df, shape = "circle", color = brewer.pal(8, "Dark2"))
    negative_cloud <- wordcloud2(data = negative_word_freq_df, shape = "circle", color = brewer.pal(8, "Reds"))
    
    # htmlwidgets로 각각의 워드 클라우드를 저장
    saveWidget(positive_cloud, "positive_cloud.html", selfcontained = TRUE)
    saveWidget(negative_cloud, "negative_cloud.html", selfcontained = TRUE)
    
    # 두 개의 htmlwidgets을 한 화면에 표시
    webshot("positive_cloud.html", file = "positive_cloud.png", delay = 10)
    webshot("negative_cloud.html", file = "negative_cloud.png", delay = 10)
    
    # magick으로 이미지 불러오기
    positive_img <- image_read("positive_cloud.png")
    negative_img <- image_read("negative_cloud.png")
    
    # ggplot으로 이미지 변환
    positive_plot <- ggplot() + 
      annotation_custom(rasterGrob(positive_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
      theme_void()
    
    negative_plot <- ggplot() + 
      annotation_custom(rasterGrob(negative_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
      theme_void()
    
    # patchwork로 두 이미지를 하나의 화면에 표시
    combined_plot <- positive_plot + negative_plot
    
    # 플롯 출력
    print(combined_plot)
    
    ```
    

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/08691aea-b5b9-4275-80cd-5d0d824962f4/864c02f5-d936-45cf-a511-161b661687f8/Untitled.png)

**[0704 점심문제]** 

cbs 라디오 그대와 여는 아침 선곡표의 노래 단어들을 워드 클라우드로 확인하세요.

- 코드
    
    ```r
    # 패키지 설치 및 라이브러리 로드
    install.packages("wordcloud2")
    install.packages("tm")  # 문자를 정제하고 전처리하기 위해 필요
    install.packages("RColorBrewer") 
    install.packages("htmlwidgets") # html 위젯을 생성하기 위해 필요 
    install.packages("webshot")  # html 파일을 png그림파일로 저장하기 위해 필요
    install.packages("magick")   # 이미지 파일을 읽고 변환하기 위한 패키지 
    install.packages("ggplot2")  
    install.packages("grid")     # 두 개의 그래프를 하나의 화면에 출력하기 위해 필요
    
    library(wordcloud2)
    library(tm)
    library(RColorBrewer)
    library(htmlwidgets)
    library(webshot)
    library(magick)
    library(ggplot2)
    library(grid)
    
    # PhantomJS 설치
    webshot::install_phantomjs() # webshot 패키지를 위해 필요한 패키지 
    
    # 텍스트 데이터 로드
    playlist_file_path <- "playlist.txt"
    playlist_txt <- readLines(playlist_file_path, encoding = "UTF-8")
    
    # 특수문자 제거 및 공백 처리
    cleaned_txt <- iconv(biden_txt, "UTF-8", "UTF-8", sub="")
    cleaned_txt <- gsub("[^[:alnum:][:space:]]", " ", cleaned_txt) #문자나 숫자 아닌 것 제거 
    cleaned_txt <- gsub("\\s+", " ", cleaned_txt) # 두 번이상의 공백문자 제거 
    
    # 텍스트를 Corpus로 변환
    corpus <- VCorpus(VectorSource(cleaned_txt))
    
    # 데이터 전처리
    corpus <- tm_map(corpus, content_transformer(tolower)) # 소문자 변환
    corpus <- tm_map(corpus, removePunctuation)            # 구두점 제거    
    corpus <- tm_map(corpus, removeNumbers)                # 숫자 제거 
    corpus <- tm_map(corpus, removeWords, stopwords("en")) # 불용어 제거 
    corpus <- tm_map(corpus, stripWhitespace)              # 공백만 있는 행 제거 
    
    # 단어 행렬 생성
    dtm <- TermDocumentMatrix(corpus)  # 단어 행렬 만들기 
    m <- as.matrix(dtm)                # 행렬형태로 변환
    word_freqs <- sort(rowSums(m), decreasing = TRUE)  # 빈도수 높은 단어로 정렬
    word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)
    word_freq_Df
    
    # 상위 100개의 단어만 선택
    word_freq_df <- head(word_freq_df, 100)
    
    # 워드 클라우드 생성
    word_cloud <- wordcloud2(data = word_freq_df, shape = "circle", color = brewer.pal(8, "Dark2"))
    
    # htmlwidgets로 워드 클라우드를 저장
    saveWidget(word_cloud, "biden_cloud.html", selfcontained = TRUE)
    
    # 워드 클라우드를 이미지로 저장
    webshot("biden_cloud.html", file = "biden_cloud.png", delay = 10)
    
    # magick으로 이미지 불러오기
    biden_img <- image_read("biden_cloud.png")
    
    # ggplot으로 이미지 변환
    biden_plot <- ggplot() + 
      annotation_custom(rasterGrob(biden_img, width=unit(1,"npc"), height=unit(1,"npc"))) + 
      theme_void()
    
    # 플롯 출력
    print(biden_plot)
    
    c드라이브-data 밑에서 png 확인 가능 
    ```
    

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/08691aea-b5b9-4275-80cd-5d0d824962f4/5dd14d4b-37e8-476c-90dc-9de149049875/Untitled.png)




